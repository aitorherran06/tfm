# ğŸ”¥ TFM â€“ AnÃ¡lisis y predicciÃ³n de riesgo de incendios en EspaÃ±a

Este repositorio contiene el cÃ³digo fuente desarrollado para el Trabajo de Fin de MÃ¡ster (TFM), cuyo objetivo es el anÃ¡lisis, integraciÃ³n y modelado de datos de incendios forestales y variables meteorolÃ³gicas en EspaÃ±a, asÃ­ como la construcciÃ³n de modelos de aprendizaje automÃ¡tico para la predicciÃ³n del riesgo de incendio a nivel provinciaâ€“dÃ­a.

El proyecto integra datos procedentes de distintas fuentes pÃºblicas y ofrece una aplicaciÃ³n interactiva desarrollada con **Streamlit** para la visualizaciÃ³n y anÃ¡lisis de resultados.

---

## ğŸ“ Estructura del repositorio

```
tfm/
â”œâ”€â”€ notebooks/              # Notebooks de anÃ¡lisis, pipeline y modelado
â”‚   â”œâ”€â”€ 00_pipeline_*.ipynb
â”‚   â”œâ”€â”€ 01_dataset_ml.ipynb
â”‚   â”œâ”€â”€ 02_modelado_ml.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ cron/                   # Scripts ejecutados mediante cron
â”‚   â”œâ”€â”€ update_firms.py     # ActualizaciÃ³n automÃ¡tica de datos FIRMS
â”‚   â””â”€â”€ update_aemet.py     # ActualizaciÃ³n automÃ¡tica de datos meteorolÃ³gicos
â”‚
â”œâ”€â”€ models/                 # Modelos de ML entrenados
â”‚   â”œâ”€â”€ modelo_rf_riesgo_aemet.joblib
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ app/                    # AplicaciÃ³n Streamlit
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ data/                   # Algunos ficheros de datos utilizados en el desarrollo
â”‚
â”œâ”€â”€ requirements.txt        # Dependencias del proyecto
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline de datos

El proyecto implementa un pipeline completo que incluye:

1. **ObtenciÃ³n de datos** desde fuentes externas pÃºblicas:
   - Incendios forestales (FIRMS, Copernicus / EFFIS)
   - InformaciÃ³n meteorolÃ³gica histÃ³rica y de predicciÃ³n (AEMET, Open-Meteo)

2. **IntegraciÃ³n espacial y temporal**:
   - AsignaciÃ³n de incendios a provincias
   - AgregaciÃ³n diaria a nivel provinciaâ€“dÃ­a

3. **Limpieza y transformaciÃ³n de datos**:
   - Tratamiento de valores nulos
   - NormalizaciÃ³n de variables
   - ConstrucciÃ³n de variables derivadas (lags y ventanas temporales)

4. **ConstrucciÃ³n del dataset de aprendizaje automÃ¡tico**, listo para entrenamiento y evaluaciÃ³n de modelos.

Todo este proceso se documenta paso a paso en los notebooks incluidos en el repositorio.

---

## â±ï¸ ActualizaciÃ³n automÃ¡tica de datos (cron jobs)

El repositorio incluye scripts que se ejecutan de forma periÃ³dica mediante tareas programadas (*cron jobs*), con el objetivo de mantener actualizados los datos utilizados en el proyecto.

Estos scripts permiten:
- Actualizar automÃ¡ticamente los datos de incendios procedentes del sistema FIRMS.
- Actualizar la informaciÃ³n meteorolÃ³gica y de predicciÃ³n futura proporcionada por AEMET.
- Almacenar los datos actualizados en la base de datos en la nube utilizada por el proyecto.

De este modo, tanto el pipeline de procesamiento como la aplicaciÃ³n de visualizaciÃ³n trabajan siempre con informaciÃ³n actualizada.

---

## ğŸ¤– Modelos de aprendizaje automÃ¡tico

Se han entrenado distintos modelos de clasificaciÃ³n binaria para la predicciÃ³n del riesgo de incendio a nivel provinciaâ€“dÃ­a, utilizando principalmente **Random Forest**.

Los modelos entrenados se incluyen en el repositorio dentro de la carpeta `models/`.  
La selecciÃ³n del modelo de aprendizaje automÃ¡tico a utilizar se realiza dinÃ¡micamente desde la aplicaciÃ³n desplegada en **Streamlit Cloud**, mediante su configuraciÃ³n, sin necesidad de modificar el cÃ³digo fuente.

---

## ğŸŒ AplicaciÃ³n Streamlit

El repositorio incluye el cÃ³digo completo de una aplicaciÃ³n desarrollada con **Streamlit**, que constituye la principal herramienta de visualizaciÃ³n del proyecto.

La aplicaciÃ³n permite:
- Explorar los datos de incendios y meteorologÃ­a.
- Visualizar resultados agregados por provincia y fecha.
- Aplicar modelos de aprendizaje automÃ¡tico y analizar predicciones.

La aplicaciÃ³n actÃºa como **capa de servicio y control**, gestionando la selecciÃ³n del modelo de ML y el acceso a los datos almacenados en la nube de forma segura.

---

## ğŸ—„ï¸ Datos

Los datos utilizados en el proyecto proceden de fuentes externas pÃºblicas.

Los datasets finales empleados tanto en el anÃ¡lisis como en el modelado se almacenan principalmente en una base de datos en la nube (**MongoDB**), desde donde son consumidos por el pipeline de procesamiento y por la aplicaciÃ³n de visualizaciÃ³n, incluyÃ©ndose tambiÃ©n una parte de los mismos en el repositorio del proyecto.

Todos los conjuntos de datos finales pueden reproducirse ejecutando el pipeline de preprocesado descrito en los notebooks, a partir de las fuentes de datos originales.

---


## ğŸ“Œ Autor

**Aitor Herran**  
Trabajo de Fin de MÃ¡ster â€“ Visual Analytics and Big Data

---

## ğŸŸ¢ Estado del proyecto

- Pipeline completo
- ActualizaciÃ³n automÃ¡tica de datos
- Modelos entrenados
- AplicaciÃ³n de visualizaciÃ³n
- Proyecto reproducible
